{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T16:42:06.994183Z",
     "start_time": "2024-07-11T16:42:06.983655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(cv.__version__)\n",
    "print(np.__version__)"
   ],
   "id": "3c62ab6df9c191e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.10.0\n",
      "1.26.4\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T03:49:01.296553Z",
     "start_time": "2024-07-01T03:49:01.293923Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1a7d728b97fe8d80",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T03:49:01.558177Z",
     "start_time": "2024-07-01T03:49:01.548166Z"
    }
   },
   "cell_type": "code",
   "source": "mp_hands = mp.solutions.hands.Hands()",
   "id": "fd98adc419e6fd3a",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T03:49:01.969477Z",
     "start_time": "2024-07-01T03:49:01.908440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img = cv.imread(\"Fred.jpg\")\n",
    "if mp_hands.process(image=img).multi_hand_landmarks:\n",
    "    print(\"True\")\n",
    "else:\n",
    "    print(\"False\")"
   ],
   "id": "5ab2e47b35c228ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T03:49:03.006285Z",
     "start_time": "2024-07-01T03:49:02.838275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img = cv.imread(\"Hand.jpg\")\n",
    "if mp_hands.process(image=img).multi_hand_landmarks:\n",
    "    print(\"True\")\n",
    "else:\n",
    "    print(\"False\")"
   ],
   "id": "6329e771f44251b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T03:49:07.010667Z",
     "start_time": "2024-07-01T03:49:03.193552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vid = cv.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    ret, frame = vid.read()\n",
    "    cv.imshow('Video', frame)\n",
    "    if cv.waitKey(1) == ord('q'):\n",
    "        break\n",
    "vid.release()\n",
    "cv.destroyAllWindows()\n"
   ],
   "id": "ba9771eaf0c01dc1",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T03:33:40.476874Z",
     "start_time": "2024-07-01T03:33:15.091740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Capture Images\n",
    "\n",
    "# Define the labels\n",
    "Labels = [\"Hallo\", \"Gut\", \"Schlecht\"]\n",
    "label_map = {\"Hallo\": 0, \"Gut\": 1, \"Schlecht\": 2}\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "# Define the capture time\n",
    "waitTime = 8\n",
    "holdTime = 1\n",
    "\n",
    "# Initialize an array to store face landmarks\n",
    "face_landmarks_array = []\n",
    "\n",
    "# Set VideoCapture to default camera\n",
    "vid = cv.VideoCapture(0)\n",
    "\n",
    "# Get the starting time\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# For each label\n",
    "for label in Labels:\n",
    "    # Create a directory for this label in the Dataset directory\n",
    "    os.makedirs(f'./Dataset/{label}', exist_ok=True)\n",
    "    while(True):\n",
    "        ret, frame = vid.read()\n",
    "        # Calculate the countdown time\n",
    "        countdown = int(waitTime - (time.time() - start))\n",
    "        # Add the countdown time to the frame\n",
    "        cv.putText(frame, \"Record for \"+ label +\" begin in \" + str(countdown), (50, 50), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv.LINE_AA)\n",
    "        cv.imshow('Video', frame)\n",
    "    \n",
    "        if cv.waitKey(1) == ord('q') or time.time() - start > waitTime:\n",
    "            break\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    # Frame counter \n",
    "    frame_num = 0\n",
    "    \n",
    "    while (True):\n",
    "        ret, frame = vid.read()\n",
    "        if ret:\n",
    "            # Convert the image from BGR to RGB\n",
    "            image = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "    \n",
    "            cv.imwrite(f'./Dataset/{label}/frame_{frame_num}.jpg', frame)\n",
    "            frame_num += 1\n",
    "        \n",
    "            # Calculate the countdown time\n",
    "            countdown = int(holdTime - (time.time() - start))\n",
    "            # Add the countdown time to the frame\n",
    "            cv.putText(frame, \"Holde Pose \" + str(countdown), (50, 50), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv.LINE_AA)\n",
    "    \n",
    "            cv.imshow('Video', frame)\n",
    "    \n",
    "            # Break the loop if 'q' is pressed or 5 seconds have passed\n",
    "            if cv.waitKey(1) == ord('q') or time.time() - start > holdTime:\n",
    "                break\n",
    "        else:\n",
    "            print(\"Video stream not available.\")\n",
    "            break\n",
    "\n",
    "vid.release()\n",
    "cv.destroyAllWindows()"
   ],
   "id": "128c6fb262f2daa5",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T03:52:10.908885Z",
     "start_time": "2024-07-01T03:52:10.744303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize MediaPipe holistic\n",
    "mp_holistic = mp.solutions.holistic.Holistic()\n",
    "\n",
    "def extract_landmarks(frame):\n",
    "    image = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image\n",
    "    results = mp_holistic.process(image)\n",
    "\n",
    "    # Get landmarks from results if available, otherwise set to zeros\n",
    "    face_landmarks = np.array([[landmark.x, landmark.y, landmark.z] for landmark in\n",
    "                               results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(\n",
    "        (468 * 3,))\n",
    "    pose_landmarks = np.array([[landmark.x, landmark.y, landmark.z] for landmark in\n",
    "                               results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(\n",
    "        (33 * 3,))\n",
    "    left_hand_landmarks = np.array([[landmark.x, landmark.y, landmark.z] for landmark in\n",
    "                                    results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(\n",
    "        (21 * 3,))\n",
    "    right_hand_landmarks = np.array([[landmark.x, landmark.y, landmark.z] for landmark in\n",
    "                                     results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(\n",
    "        (21 * 3,))\n",
    "\n",
    "    # Concatenate all landmarks into a single array\n",
    "    all_landmarks = np.concatenate((face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks))\n",
    "    return all_landmarks\n",
    "\n",
    "mp_holistic.close()"
   ],
   "id": "e8c077a54d92302f",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T03:52:25.318866Z",
     "start_time": "2024-07-01T03:52:13.557441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mp_holistic = mp.solutions.holistic.Holistic()\n",
    "\n",
    "# Initialize a list to hold dataset\n",
    "dataset = []\n",
    "\n",
    "# For every labeled folder in the Dataset directory\n",
    "for label in os.listdir('./Dataset/'):\n",
    "    print(\"Checking: \" + label)\n",
    "    if os.path.isdir(f'./Dataset/{label}'):\n",
    "        # For every image file in the folder\n",
    "        for filename in os.listdir(f'./Dataset/{label}'):\n",
    "\n",
    "            # Read the image file\n",
    "            image = cv.imread(f'./Dataset/{label}/{filename}')\n",
    "\n",
    "            all_landmarks = extract_landmarks(image)\n",
    "                \n",
    "            dataset.append((torch.tensor(all_landmarks), label))\n",
    "\n",
    "mp_holistic.close()"
   ],
   "id": "4d92af99a02af57a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking: Gut\n",
      "Checking: Hallo\n",
      "Checking: Schlecht\n"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T03:38:08.366337Z",
     "start_time": "2024-07-01T03:38:08.361432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "InFeatures=1629 \n",
    "OutClasses=3\n",
    "\n",
    "class LandmarksClassifier(nn.Module):\n",
    "    def __init__(self, in_feat=InFeatures, hiddenlayer=50, num_classes=OutClasses):  # Start with three classes and adjust as your project evolves.\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_feat, hiddenlayer)\n",
    "        self.layer2 = nn.Linear(hiddenlayer, hiddenlayer)\n",
    "        self.layer3 = nn.Linear(hiddenlayer, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x)  # No need activation here as normally the loss function will take care of it\n",
    "        return x"
   ],
   "id": "e0a4546fd89f434e",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T03:38:10.943158Z",
     "start_time": "2024-07-01T03:38:10.937996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 128\n",
    "\n",
    "landmarks_list = [item[0] for item in dataset]\n",
    "labels_list = [item[1] for item in dataset]\n",
    "\n",
    "landmarks_tensor = torch.stack(landmarks_list)\n",
    "labels_list = [label_map[item[1]] for item in dataset]\n",
    "labels_tensor = torch.tensor(labels_list)\n",
    "\n",
    "tensor_dataset = TensorDataset(landmarks_tensor, labels_tensor)\n",
    "data_loader = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "id": "4ee9f91c799ba4c5",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T03:38:11.376658Z",
     "start_time": "2024-07-01T03:38:11.373635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lr = 0.0005\n",
    "epochs = 200"
   ],
   "id": "76f89e1a6fd205c0",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T03:38:11.837297Z",
     "start_time": "2024-07-01T03:38:11.755163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup the model, criterion, and optimizer\n",
    "model = LandmarksClassifier(InFeatures, hiddenlayer=50, num_classes=len(Labels))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 25\n",
    "\n",
    "# Start training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(data_loader):\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.float()  # change it back to float\n",
    "        labels_one_hot = F.one_hot(labels.to(torch.int64), num_classes=3)  # use labels as int64 for one-hot function\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels_one_hot.float())\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {correct / len(labels):.2f}')\n",
    "\n",
    "print('Finished Training')"
   ],
   "id": "947f6e51992768f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "torch.Size([121, 1629])\n",
      "Finished Training\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T03:38:14.517720Z",
     "start_time": "2024-07-01T03:38:14.515146Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e60f419e34ef9c2f",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T03:38:14.796149Z",
     "start_time": "2024-07-01T03:38:14.791149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_landmarks(landmarks):\n",
    "    # Converts landmarks into a tensor and adds another dimension to fit \n",
    "    # the (batch_size, num_features). \n",
    "    landmarks = torch.tensor(landmarks)\n",
    "    landmarks = landmarks.unsqueeze_(0).float()  # convert to (1, num_features)\n",
    "    return landmarks\n",
    "\n",
    "def predict_landmark_class(landmarks, model):\n",
    "    # Use your classifier model to predict the landmark class.\n",
    "    output = model(landmarks)\n",
    "    _, pred = torch.max(output, 1)  # get the index of the max log-probability\n",
    "    return pred.item()"
   ],
   "id": "50ecc68bd140e8af",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T03:40:26.802701Z",
     "start_time": "2024-07-01T03:40:21.767048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mp_holistic = mp.solutions.holistic.Holistic()\n",
    "model.eval()\n",
    "\n",
    "vid = cv.VideoCapture(0)\n",
    "\n",
    "while (True):\n",
    "    ret, frame = vid.read()\n",
    "    landmarks = extract_landmarks(frame)\n",
    "    landmarks = process_landmarks(landmarks)\n",
    "    prediction = predict_landmark_class(landmarks, model)\n",
    "    \n",
    "    cv.putText(frame, \"Predicted class is \" + str(inv_label_map[prediction]), (50, 50), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv.LINE_AA)\n",
    "\n",
    "    cv.imshow('Video', frame)\n",
    "\n",
    "    if cv.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "vid.release()\n",
    "cv.destroyAllWindows()\n",
    "mp_holistic.close()"
   ],
   "id": "c83876a3cc76cbf0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n",
      "torch.Size([1, 1629])\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bc515db1fbef9eeb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
